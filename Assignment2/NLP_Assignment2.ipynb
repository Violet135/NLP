{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment-2_.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rhRj0trlq7JN",
        "XYLiP7bHqpTa",
        "uhywMTY_-zR1",
        "ivId95NOhh3m"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7SQfvv_qfnc"
      },
      "source": [
        "# Assignment 2\n",
        "Text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhRj0trlq7JN"
      },
      "source": [
        "# Install and Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ITOs-47YSd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm5n-soOdCAs"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYLiP7bHqpTa"
      },
      "source": [
        "# Connect with Google Drive and import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJnmj7gEhQOL",
        "outputId": "af428f2c-f919-488d-ad7d-acad4d2236da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loading stopwords\n",
        "!wget https://raw.githubusercontent.com/kharazi/persian-stopwords/master/nonverbal\n",
        "!wget https://raw.githubusercontent.com/kharazi/persian-stopwords/master/verbal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-05 13:06:53--  https://raw.githubusercontent.com/kharazi/persian-stopwords/master/nonverbal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1291 (1.3K) [text/plain]\n",
            "Saving to: ‘nonverbal’\n",
            "\n",
            "nonverbal           100%[===================>]   1.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-05 13:06:53 (51.6 MB/s) - ‘nonverbal’ saved [1291/1291]\n",
            "\n",
            "--2020-11-05 13:06:53--  https://raw.githubusercontent.com/kharazi/persian-stopwords/master/verbal\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283 [text/plain]\n",
            "Saving to: ‘verbal’\n",
            "\n",
            "verbal              100%[===================>]     283  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-05 13:06:54 (12.9 MB/s) - ‘verbal’ saved [283/283]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kwuBS9wjo93",
        "outputId": "3a2a270e-f5a3-47f8-c326-c8c1d912e1a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "FOLDERNAME = \"Courses/NLP/Assignment2/\"\n",
        "%cd drive/My\\ Drive\n",
        "%cp -r $FOLDERNAME ../../\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkYcrwVwtM2Q",
        "outputId": "81e7db3c-1584-4d13-86f0-c11dda602540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unrar x Assignment2/datasets/News.rar Assignment2/datasets/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Assignment2/datasets/News.rar\n",
            "\n",
            "Creating    Assignment2/datasets/News                                 OK\n",
            "Extracting  Assignment2/datasets/News/train.csv                          \b\b\b\b  5%\b\b\b\b 11%\b\b\b\b 16%\b\b\b\b 22%\b\b\b\b 27%\b\b\b\b 33%\b\b\b\b 38%\b\b\b\b 44%\b\b\b\b 49%\b\b\b\b 55%\b\b\b\b 61%\b\b\b\b 66%\b\b\b\b 72%\b\b\b\b 77%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b\b  OK \n",
            "Extracting  Assignment2/datasets/News/test.csv                           \b\b\b\b 89%\b\b\b\b 95%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlGOZ4hx1sT5"
      },
      "source": [
        "train = pd.read_csv('Assignment2/datasets/News/train.csv', sep='\\t', encoding='utf-8')\n",
        "test = pd.read_csv('Assignment2/datasets/News/test.csv', sep='\\t', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CAKTug69Ks1",
        "outputId": "416defec-2e2a-4d10-9b49-ebfb11f1f262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>title</th>\n",
              "      <th>code_news</th>\n",
              "      <th>category</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/fa/news/6276899/اسفراین-جولانگاه-مقدونیان-تصا...</td>\n",
              "      <td>اسفراین؛ جولانگاه مقدونیان + تصاویر</td>\n",
              "      <td>کد خبر: ۶۲۷۶۸۹۹</td>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>تاریخ انتشار: ۲۱ مهر ۱۳۹۶ - ۰۷:۰۴</td>\n",
              "      <td>به گزارش خبرنگار حوزه میراث و گردشگری گروه فر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/fa/news/4626291/اجرای-قطعات-سلام-آقا-بدون-انگ...</td>\n",
              "      <td>اجرای قطعات \"سلام آقا\" بدون انگیزه مالی و با ا...</td>\n",
              "      <td>کد خبر: ۴۶۲۶۲۹۱</td>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>تاریخ انتشار: ۱۹ آبان ۱۳۹۲ - ۱۰:۰۰</td>\n",
              "      <td>به گزارش حوزه موسیقی باشگاه خبرنگاران، مهدی ی...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/fa/news/4632846/حکایت-سیاریحون-از-نهضت-میرزا-...</td>\n",
              "      <td>حکایت \"سیاریحون\" از نهضت میرزا کوچک خان جنگلی ...</td>\n",
              "      <td>کد خبر: ۴۶۳۲۸۴۶</td>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>تاریخ انتشار: ۲۵ آبان ۱۳۹۲ - ۱۳:۴۵</td>\n",
              "      <td>به گزارش حوزه تئاتر باشگاه خبرنگاران به نقل ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/fa/news/5058438/گفتگو-با-کارگردان-سریال-شهید-...</td>\n",
              "      <td>گفتگو با کارگردان سریال شهید مدرس/قدرت بیان فو...</td>\n",
              "      <td>کد خبر: ۵۰۵۸۴۳۸</td>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>تاریخ انتشار: ۱۰ آذر ۱۳۹۳ - ۱۰:۵۴</td>\n",
              "      <td>به گزارش خبرنگار رادیو تلویزیون باشگاه خبرنگا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/fa/news/6668241/گزارش-تصویری-مراسم-عزاداری-شب...</td>\n",
              "      <td>گزارش تصویری مراسم عزاداری شب ششم محرم ۹۷/ حضو...</td>\n",
              "      <td>کد خبر: ۶۶۶۸۲۴۱</td>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>تاریخ انتشار: ۲۵ شهريور ۱۳۹۷ - ۱۰:۳۵</td>\n",
              "      <td>به گزارش خبرنگار تکیه حسینی گروه فرهنگی باشگا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link  ...                                               text\n",
              "0  /fa/news/6276899/اسفراین-جولانگاه-مقدونیان-تصا...  ...   به گزارش خبرنگار حوزه میراث و گردشگری گروه فر...\n",
              "1  /fa/news/4626291/اجرای-قطعات-سلام-آقا-بدون-انگ...  ...   به گزارش حوزه موسیقی باشگاه خبرنگاران، مهدی ی...\n",
              "2  /fa/news/4632846/حکایت-سیاریحون-از-نهضت-میرزا-...  ...   به گزارش حوزه تئاتر باشگاه خبرنگاران به نقل ا...\n",
              "3  /fa/news/5058438/گفتگو-با-کارگردان-سریال-شهید-...  ...   به گزارش خبرنگار رادیو تلویزیون باشگاه خبرنگا...\n",
              "4  /fa/news/6668241/گزارش-تصویری-مراسم-عزاداری-شب...  ...   به گزارش خبرنگار تکیه حسینی گروه فرهنگی باشگا...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2gNjQO8Gy3-"
      },
      "source": [
        "Concatenating news text and title and replacing the empty ones with 'nan'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON9xOs1pcB_4",
        "outputId": "85280efa-08f5-4e28-f974-c28d627ead11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train[\"news\"] = train[\"text\"].fillna(' ') + train[\"title\"].fillna(' ')\n",
        "print(len(train[train['news']=='  ']))\n",
        "# print(train[train['news']=='  '])\n",
        "# train.drop()\n",
        "train = train.drop(['text', 'title', 'date', 'link', 'code_news'], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkhDQYzkHPxW"
      },
      "source": [
        "Remove 'nan' items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQyHSMEDszY4",
        "outputId": "48c95882-7c38-4058-e23b-5803550927d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "train[train['news']=='  '] = np.nan\n",
        "train.dropna(inplace = True)\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>news</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش خبرنگار حوزه میراث و گردشگری گروه فر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش حوزه موسیقی باشگاه خبرنگاران، مهدی ی...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش حوزه تئاتر باشگاه خبرنگاران به نقل ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش خبرنگار رادیو تلویزیون باشگاه خبرنگا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش خبرنگار تکیه حسینی گروه فرهنگی باشگا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117187</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>به گزارش\\r\\nخبرنگار حوزه فوتبال و فوتسال گروه...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117188</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>به گزارش خبرنگار\\r\\nورزشی باشگاه خبرنگاران؛ ر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117189</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>منچستریونایتد مذاکره با گواردیولا را تکذیب کرد</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117190</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>به گزارش  خبرنگار فوتبال و فوتسال گروه ورزشی ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117191</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>*برنامه رقابت های  ملی پوشان کاروان ایران : گ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>117161 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               news\n",
              "0       فرهنگی هنری   به گزارش خبرنگار حوزه میراث و گردشگری گروه فر...\n",
              "1       فرهنگی هنری   به گزارش حوزه موسیقی باشگاه خبرنگاران، مهدی ی...\n",
              "2       فرهنگی هنری   به گزارش حوزه تئاتر باشگاه خبرنگاران به نقل ا...\n",
              "3       فرهنگی هنری   به گزارش خبرنگار رادیو تلویزیون باشگاه خبرنگا...\n",
              "4       فرهنگی هنری   به گزارش خبرنگار تکیه حسینی گروه فرهنگی باشگا...\n",
              "...             ...                                                ...\n",
              "117187        ورزشی   به گزارش\\r\\nخبرنگار حوزه فوتبال و فوتسال گروه...\n",
              "117188        ورزشی   به گزارش خبرنگار\\r\\nورزشی باشگاه خبرنگاران؛ ر...\n",
              "117189        ورزشی     منچستریونایتد مذاکره با گواردیولا را تکذیب کرد\n",
              "117190        ورزشی   به گزارش  خبرنگار فوتبال و فوتسال گروه ورزشی ...\n",
              "117191        ورزشی   *برنامه رقابت های  ملی پوشان کاروان ایران : گ...\n",
              "\n",
              "[117161 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5nvrp6e23xR",
        "outputId": "4a298640-594f-44e7-d0cf-ca8f3a949926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "train[train['category']=='category'] = np.nan\n",
        "train.dropna(inplace = True)\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>news</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش خبرنگار حوزه میراث و گردشگری گروه فر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش حوزه موسیقی باشگاه خبرنگاران، مهدی ی...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش حوزه تئاتر باشگاه خبرنگاران به نقل ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش خبرنگار رادیو تلویزیون باشگاه خبرنگا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>فرهنگی هنری</td>\n",
              "      <td>به گزارش خبرنگار تکیه حسینی گروه فرهنگی باشگا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117187</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>به گزارش\\r\\nخبرنگار حوزه فوتبال و فوتسال گروه...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117188</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>به گزارش خبرنگار\\r\\nورزشی باشگاه خبرنگاران؛ ر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117189</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>منچستریونایتد مذاکره با گواردیولا را تکذیب کرد</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117190</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>به گزارش  خبرنگار فوتبال و فوتسال گروه ورزشی ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117191</th>\n",
              "      <td>ورزشی</td>\n",
              "      <td>*برنامه رقابت های  ملی پوشان کاروان ایران : گ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>117153 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               news\n",
              "0       فرهنگی هنری   به گزارش خبرنگار حوزه میراث و گردشگری گروه فر...\n",
              "1       فرهنگی هنری   به گزارش حوزه موسیقی باشگاه خبرنگاران، مهدی ی...\n",
              "2       فرهنگی هنری   به گزارش حوزه تئاتر باشگاه خبرنگاران به نقل ا...\n",
              "3       فرهنگی هنری   به گزارش خبرنگار رادیو تلویزیون باشگاه خبرنگا...\n",
              "4       فرهنگی هنری   به گزارش خبرنگار تکیه حسینی گروه فرهنگی باشگا...\n",
              "...             ...                                                ...\n",
              "117187        ورزشی   به گزارش\\r\\nخبرنگار حوزه فوتبال و فوتسال گروه...\n",
              "117188        ورزشی   به گزارش خبرنگار\\r\\nورزشی باشگاه خبرنگاران؛ ر...\n",
              "117189        ورزشی     منچستریونایتد مذاکره با گواردیولا را تکذیب کرد\n",
              "117190        ورزشی   به گزارش  خبرنگار فوتبال و فوتسال گروه ورزشی ...\n",
              "117191        ورزشی   *برنامه رقابت های  ملی پوشان کاروان ایران : گ...\n",
              "\n",
              "[117153 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b6ZRE6WHVSX"
      },
      "source": [
        "Split data to train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNXuDWuM2n6e",
        "outputId": "3938be60-8b67-4169-8f26-7d635dfdf22e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train['news'], train['category'], test_size=0.3, random_state=42)\n",
        "\n",
        "X_train.reindex(np.arange(X_train.shape[0])) \n",
        "y_train.reindex(np.arange(y_train.shape[0])) \n",
        "X_val.reindex(np.arange(X_val.shape[0])) \n",
        "y_val.reindex(np.arange(y_val.shape[0])) \n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(82007,)\n",
            "(82007,)\n",
            "(35146,)\n",
            "(35146,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE4p-yim_-6k"
      },
      "source": [
        "# Corpus class\n",
        "It contains cleaning text, and it stores corpus, dictionary and others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3RjiwRkcl_3"
      },
      "source": [
        "## The Corpus class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjaYig7ucfB7"
      },
      "source": [
        "import re\n",
        "\n",
        "class Corpus():\n",
        "\n",
        "    def __init__(self, data, replace_frequent=True, train = True, char = False, frequent_number = 10000):\n",
        "        self.stopwords, self.chars = self.load_requirements()\n",
        "        self.corpus = self.clean(data)\n",
        "        self.dictionary = self.create_dictionary()\n",
        "        self.word2index = {'PAD':0, 'UNK':1}   \n",
        "        self.index2word = {0:'PAD', 1:'UNK'}\n",
        "        if train:\n",
        "            self.most_frequent_words = self.most_frequent(number = frequent_number)\n",
        "            self.UNK_count = 0\n",
        "            if replace_frequent:\n",
        "                self.UNK_count = self.replace_with_UNK()\n",
        "            self.create_dictionaries()\n",
        "        if char:    \n",
        "            self.dictionary_char = self.create_dictionary_char()\n",
        "            self.char2index, self.index2char = self.create_dictionaries_char()\n",
        "\n",
        "    def replace_numbers(self, data):\n",
        "        data = re.sub('((2[0-4]|[0-1]?[0-9]):([0-5][0-9]|[0-9]))', 'N', data)\n",
        "        data = re.sub('((۲[۰-۴]|[۰-۱]?[۰-۹]):([۰-۵][۰-۹]|[۰-۹]))', 'N', data)\n",
        "        data = re.sub(r'\\d+', 'N', data) \n",
        "        return data\n",
        "\n",
        "    def remove_stopwords(self, data):\n",
        "        new_data = []\n",
        "        for i in data:\n",
        "            if i not in self.stopwords:\n",
        "              new_data.append(i)\n",
        "        return new_data\n",
        "\n",
        "    def clean(self, data):\n",
        "        corpus =[]\n",
        "        for i in range(data.shape[0]):\n",
        "            news = data.iloc[i]\n",
        "            news = re.sub(r'\\\\','',news)\n",
        "            news = news = re.sub(r'\\xa0','',news)\n",
        "            news = re.sub(r'[a-zA-Z]+','',news)\n",
        "            news = news.replace('_','')\n",
        "            news = news.replace('ـ','')\n",
        "            news = re.sub(r'[^\\w\\d.؟\\s]+','',news)\n",
        "            news = self.replace_numbers(news)\n",
        "            news = self.character_refinement(news)\n",
        "            news = news.replace('.',' .')\n",
        "            news = news.replace('؟',' ؟')\n",
        "            news = \" \".join(news.split()).split(' ')\n",
        "            news = self.remove_stopwords(news)\n",
        "            corpus.append(news)\n",
        "        return corpus\n",
        "        \n",
        "    def create_dictionary(self):\n",
        "        dictionary = {'PAD':0}\n",
        "        for news in self.corpus:\n",
        "            for word in news:\n",
        "                if word in dictionary:\n",
        "                    dictionary[word] += 1\n",
        "                else: dictionary[word] = 1\n",
        "        return dictionary\n",
        "\n",
        "    def number_of_all_tokens(self):\n",
        "        count = sum([len(listElem) for listElem in self.corpus])\n",
        "        return count\n",
        "    \n",
        "    def number_of_all_unique_tokens(self):\n",
        "        return len(self.dictionary) \n",
        "\n",
        "    def most_frequent(self, number = 50):\n",
        "        sorted_dict = sorted(self.dictionary.items(), key=lambda x: x[1], reverse=True)\n",
        "        most_frequent = sorted_dict[:number]\n",
        "        most_frequent = pd.DataFrame(np.array([list(item) for item in most_frequent]))\n",
        "        return set(most_frequent[0].values.tolist())\n",
        "\n",
        "    def replace_with_UNK(self):\n",
        "        count = 0\n",
        "        for index1, news in enumerate(self.corpus):\n",
        "            for index2, word in enumerate(news):\n",
        "                if word not in self.most_frequent_words:\n",
        "                    self.corpus[index1][index2]='UNK'\n",
        "                    count += 1\n",
        "        return count\n",
        "\n",
        "    def create_dictionaries(self):\n",
        "        counter = 2\n",
        "        for word in sorted(self.dictionary):\n",
        "            if word in self.most_frequent_words:\n",
        "                self.word2index[word]=counter\n",
        "                self.index2word[counter]=word\n",
        "                counter += 1\n",
        "\n",
        "    def create_dictionary_char(self):\n",
        "        chars ={}\n",
        "        for word in self.dictionary.keys():\n",
        "            for ch in word:\n",
        "                if ch in chars.keys():\n",
        "                    chars[ch] += 1\n",
        "                else:\n",
        "                    chars[ch] = 1\n",
        "        return chars\n",
        "\n",
        "    def load_requirements(self):\n",
        "        f1 = open(\"nonverbal\", \"r\")\n",
        "        f2 = open(\"verbal\", \"r\")\n",
        "        stopwords1 = f1.read()\n",
        "        stopwords1 = (stopwords1.split('\\n'))\n",
        "        stopwords2 = f2.read()\n",
        "        stopwords2 = (stopwords2.split('\\n'))\n",
        "        stopwords = set(stopwords1+stopwords2)\n",
        "        chars = pd.read_csv('Assignment2/datasets/char_replacement.txt', sep=',', encoding='utf-8', names=[0,1]).values.tolist()\n",
        "        chars_dic = {each[0]:each[1][1] for each in chars}     \n",
        "        return stopwords, chars_dic\n",
        "\n",
        "    def character_refinement(self, news):\n",
        "        for ch in self.chars:\n",
        "            news = news.replace(ch, self.chars[ch])\n",
        "        return news\n",
        "\n",
        "    def create_dictionaries_char(self):\n",
        "        char2index = {'UNK':0}\n",
        "        index2char = {0:'UNK'}\n",
        "        counter = 1\n",
        "        for char in sorted(self.dictionary_char):\n",
        "            char2index[char] = counter\n",
        "            index2char[counter] = char\n",
        "            counter += 1\n",
        "        return char2index, index2char\n",
        "\n",
        "    def tokenize(self, level=0, data = 1):\n",
        "        tokenized = []\n",
        "        if type(data) == int:\n",
        "            data = self.corpus\n",
        "        if level==0 :\n",
        "            for news in data:\n",
        "                news_ =[]\n",
        "                for word in news:\n",
        "                    if word in self.word2index:\n",
        "                        news_.append(self.word2index[word])\n",
        "                    else:\n",
        "                        news_.append(self.word2index['UNK'])\n",
        "                tokenized.append(news_)\n",
        "        return tokenized\n",
        "\n",
        "    def vectorize(self, data, vectorize_type='BoW'):\n",
        "        \n",
        "        if vectorize_type=='BoW':\n",
        "            # Create bag of words\n",
        "            BoW = []\n",
        "            for news in data:\n",
        "                bow = (np.zeros(len(self.word2index))).tolist()\n",
        "                for word in news:\n",
        "                    if word in self.word2index:\n",
        "                        bow[self.word2index[word]] += 1\n",
        "                    else:\n",
        "                        bow[self.word2index['UNK']] += 1\n",
        "                BoW.append(bow)\n",
        "            print(len(BoW))\n",
        "            print(len(BoW[0]))\n",
        "            return BoW\n",
        "        \n",
        "        else:\n",
        "            print('Not yet!')\n",
        "            return [0]\n",
        "\n",
        "    def less_than_ave(self, y, add_pad = True):\n",
        "        # compute average, delete long sentences, add_pading\n",
        "        \n",
        "        count = sum([len(listElem) for listElem in self.corpus])\n",
        "        average = (count/(len(self.corpus)))\n",
        "        remain_y = []\n",
        "        remain_X = []\n",
        "        for index, news in enumerate(self.corpus):\n",
        "            if len(news) <= average:\n",
        "                remain_y.append(y.iloc[index])\n",
        "                remain_X.append(self.corpus[index])\n",
        "\n",
        "        if add_pad: # Add PAD\n",
        "            average = round(average)\n",
        "            for index, news in enumerate(remain_X):\n",
        "                pad_count = (average-len(news))\n",
        "                remain_X[index] = remain_X[index] + ['PAD']*pad_count\n",
        "                self.dictionary['PAD'] += pad_count\n",
        "\n",
        "        return remain_X, remain_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxqBxHW_rTEa",
        "outputId": "33bbb9cc-a6e5-4fba-f8ee-9b2ffd3763bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "row = 100\n",
        "c1 = Corpus(X_train.iloc[:row], frequent_number=1000)\n",
        "X_new, y_new = c1.less_than_ave(y_train.iloc[:row])\n",
        "tokenized = c1.tokenize(level=0, data = X_new)\n",
        "BoW = c1.vectorize(X_train.iloc[:row])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "1002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhywMTY_-zR1"
      },
      "source": [
        "## Create the dictionary and dictionary file for chars\n",
        "* These cells are messy and doesn't need to be run\n",
        "* They helped me to replace some characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-XFySb9dbcC",
        "outputId": "daddf8ab-75e6-412f-a47f-ad216c92c600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new = X_train#.iloc[:1000]\n",
        "new_y = y_train#.iloc[:1000]\n",
        "c = Corpus(new)\n",
        "c.dictionary_char.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['گ', 'ز', 'ا', 'ر', 'ش', 'خ', 'ب', 'ن', 'س', 'ی', 'ه', 'و', 'ل', 'ق', 'م', 'ح', 'پ', 'د', 'ئ', 'ک', 'N', 'ف', 'ت', '.', 'ج', 'ع', 'چ', 'ط', 'ض', 'غ', 'ص', 'ظ', 'ذ', 'آ', 'ث', 'ي', 'ژ', 'ك', 'ؤ', 'أ', 'ء', 'ـ', '؟', 'ۀ', 'ى', 'إ', 'ة', 'ˈ', 'é', 'ﻫ', 'ﻤ', 'ﻴ', 'ﻠ', 'ﺘ', 'ﻮ', 'ﻥ', 'ﺸ', 'ﻪ', 'ﺑ', 'ﻬ', 'ﺮ', 'ﻳ', 'ﻦ', 'ﻧ', 'ﺠ', 'ﺭ', 'ﺍ', 'ﯽ', 'ﺗ', 'ﮐ', 'ﺴ', 'ﺐ', 'ﺩ', '½', '¼', 'ہ', 'ۆ', 'ﻣ', 'ﻨ', 'ﮕ', 'ﺳ', 'ﻞ', 'ﯿ', 'ﻝ', 'ﻌ', 'ﺯ', 'ﮑ', 'ﻼ', 'ﺕ', 'ﺒ', 'ﻓ', 'ں', 'ﺎ', 'ﻟ', 'ﺷ', 'ﯾ', 'ﯼ', 'ﻄ', 'ﻘ', 'ﺣ', 'ﻏ', 'ﻕ', 'ﺪ', 'ﺧ', 'ﺝ', 'ﻋ', 'ﺖ', 'ﻃ', 'ﺡ', 'ﻭ', 'ﺁ', 'ﻔ', 'ﺰ', 'ﺟ', 'ﻡ', 'ﻩ', 'ﭼ', 'ﭘ', 'ﺞ', 'ﻗ', 'ﻇ', 'ﻒ', 'ﮔ', 'ﻛ', 'ە', 'ۯ', 'ā', 'М', 'о', 'н', 'г', 'л', 'у', 'с', 'Ө', 'и', 'й', 'Б', 'а', 'Ч', 'б', 'Э', 'р', 'д', 'э', 'т', 'ó', 'ﺤ', 'ﻜ', 'ﻢ', 'ﺛ', 'ﻉ', 'ﻀ', 'ﺻ', 'ﺢ', 'ﻂ', 'č', 'ۃ', 'ï', 'â', 'ğ', 'ھ', 'ﺏ', 'ﮎ', 'ﺨ', 'ﺼ', 'ﺹ', 'ﺬ', 'ﺵ', 'ﻯ', 'æ', 'ɔ', 'ɪ', 'ﻈ', 'ﻖ', 'ä', 'ć', 'á', 'è', 'ö', 'ﺿ', 'ﺲ', 'ﺋ', 'ﮏ', 'ڗ', 'ﺶ', 'ﻲ', 'ã', 'ē', 'í', 'ə', 'Á', 'ñ', 'ﺆ', 'ﭽ', 'ﻐ', 'ﺱ', 'ﺌ', 'ﻍ', 'ﺄ', 'ﻻ', 'ﮋ', 'ﻊ', 'ﭗ', 'ﺜ', 'ﻑ', 'α', 'ﺥ', 'ﻙ', 'ﭙ', 'ﺦ', 'ﺅ', 'ﭻ', 'ﭖ', 'ﻁ', 'ﮒ', 'ī', 'ü', 'ﷲ', 'ł', 'ø', 'ګ', '²', 'ﻰ'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38VUxvSvk6WG",
        "outputId": "21ff1574-2cba-4f76-cc41-c6da9117a990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "FOLDERNAME = \"Courses/NLP/Assignment2/datasets\"\n",
        "%cd drive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "976BdfSIp3ks"
      },
      "source": [
        "file_char = open(\"Courses/NLP/Assignment2/datasets/char_replace.txt\", \"r\")\n",
        "charss = file_char.read()\n",
        "charss = (charss.split('\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hXJPONUrjz1",
        "outputId": "9f73a99f-d913-4751-a4ad-ebfa831c6b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd ../.."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gslZripgrHZ6"
      },
      "source": [
        "file_char = open(\"chars.txt\", \"r\")\n",
        "chars_old = file_char.read()\n",
        "chars_old = (chars_old.split('\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XACrOprSroPZ",
        "outputId": "c596c9e6-784d-49f2-fbaa-a9965ece3762",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(len(charss)):\n",
        "    print(charss[i], chars_old[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "گ گ\n",
            "ز ز\n",
            "ا ا\n",
            "ر ر\n",
            "ش ش\n",
            "خ خ\n",
            "ب ب\n",
            "ن ن\n",
            "س س\n",
            "ی ی\n",
            "ه ه\n",
            "و و\n",
            "ل ل\n",
            "ق ق\n",
            "م م\n",
            "ح ح\n",
            "پ پ\n",
            "د د\n",
            "ی ئ\n",
            "ک ک\n",
            "N N\n",
            "ف ف\n",
            "ت ت\n",
            ". .\n",
            "ج ج\n",
            "ع ع\n",
            "چ چ\n",
            "ط ط\n",
            "ض ض\n",
            "غ غ\n",
            "ص ص\n",
            "ظ ظ\n",
            "ذ ذ\n",
            "آ آ\n",
            "ث ث\n",
            "ی ي\n",
            "ژ ژ\n",
            "ک ك\n",
            "و ؤ\n",
            "ا أ\n",
            "  ء\n",
            "  ـ\n",
            "؟ ؟\n",
            "ه ۀ\n",
            "ى ى\n",
            "ا إ\n",
            "ه ة\n",
            "  ˈ\n",
            "  é\n",
            "ه ﻫ\n",
            "م ﻤ\n",
            "ی ﻴ\n",
            "ل ﻠ\n",
            "ت ﺘ\n",
            "و ﻮ\n",
            "ن ﻥ\n",
            "ش ﺸ\n",
            "ه ﻪ\n",
            "ب ﺑ\n",
            "ه ﻬ\n",
            "ر ﺮ\n",
            "ی ﻳ\n",
            "ن ﻦ\n",
            "ن ﻧ\n",
            "ج ﺠ\n",
            "ر ﺭ\n",
            "ا ﺍ\n",
            "ی ﯽ\n",
            "ت ﺗ\n",
            "ک ﮐ\n",
            "س ﺴ\n",
            "ب ﺐ\n",
            "د ﺩ\n",
            "N ½\n",
            "N ¼\n",
            "ه ہ\n",
            "و ۆ\n",
            "م ﻣ\n",
            "ن ﻨ\n",
            "گ ﮕ\n",
            "س ﺳ\n",
            "ل ﻞ\n",
            "ی ﯿ\n",
            "ل ﻝ\n",
            "ع ﻌ\n",
            "ز ﺯ\n",
            "ک ﮑ\n",
            "لا ﻼ\n",
            "ت ﺕ\n",
            "ب ﺒ\n",
            "ف ﻓ\n",
            "  ں\n",
            "ا ﺎ\n",
            "ل ﻟ\n",
            "ش ﺷ\n",
            "ی ﯾ\n",
            "ی ﯼ\n",
            "ط ﻄ\n",
            "ق ﻘ\n",
            "ح ﺣ\n",
            "غ ﻏ\n",
            "ق ﻕ\n",
            "د ﺪ\n",
            "خ ﺧ\n",
            "ج ﺝ\n",
            "ع ﻋ\n",
            "ت ﺖ\n",
            "ط ﻃ\n",
            "ح ﺡ\n",
            "و ﻭ\n",
            "آ ﺁ\n",
            "ف ﻔ\n",
            "ز ﺰ\n",
            "ج ﺟ\n",
            "م ﻡ\n",
            "ه ﻩ\n",
            "چ ﭼ\n",
            "پ ﭘ\n",
            "ج ﺞ\n",
            "ق ﻗ\n",
            "ظ ﻇ\n",
            "ف ﻒ\n",
            "گ ﮔ\n",
            "ک ﻛ\n",
            "ه ە\n",
            "ر ۯ\n",
            "  ā\n",
            "  М\n",
            "  о\n",
            "  н\n",
            "  г\n",
            "  л\n",
            "  у\n",
            "  с\n",
            "  Ө\n",
            "  и\n",
            "  й\n",
            "  Б\n",
            "  а\n",
            "  Ч\n",
            "  б\n",
            "  Э\n",
            "  р\n",
            "  д\n",
            "  э\n",
            "  т\n",
            "  ó\n",
            "ح ﺤ\n",
            "ک ﻜ\n",
            "م ﻢ\n",
            "ژ ﺛ\n",
            "ع ﻉ\n",
            "ض ﻀ\n",
            "ص ﺻ\n",
            "ح ﺢ\n",
            "ط ﻂ\n",
            "  č\n",
            "ه ۃ\n",
            "  ï\n",
            "  â\n",
            "  ğ\n",
            "ه ھ\n",
            "ب ﺏ\n",
            "ک ﮎ\n",
            "خ ﺨ\n",
            "ص ﺼ\n",
            "ص ﺹ\n",
            "ذ ﺬ\n",
            "ش ﺵ\n",
            "ی ﻯ\n",
            "  æ\n",
            "  ɔ\n",
            "  ɪ\n",
            "ظ ﻈ\n",
            "ق ﻖ\n",
            "  ä\n",
            "  ć\n",
            "  á\n",
            "  è\n",
            "  ö\n",
            "ض ﺿ\n",
            "س ﺲ\n",
            "ی ﺋ\n",
            "ک ﮏ\n",
            "ژ ڗ\n",
            "ش ﺶ\n",
            "ی ﻲ\n",
            "  ã\n",
            "  ē\n",
            "  í\n",
            "  ə\n",
            "  Á\n",
            "  ñ\n",
            "و ﺆ\n",
            "چ ﭽ\n",
            "غ ﻐ\n",
            "س ﺱ\n",
            "ی ﺌ\n",
            "غ ﻍ\n",
            "ا ﺄ\n",
            "لا ﻻ\n",
            "ژ ﮋ\n",
            "ع ﻊ\n",
            "پ ﭗ\n",
            "ث ﺜ\n",
            "ف ﻑ\n",
            "  α\n",
            "خ ﺥ\n",
            "ک ﻙ\n",
            "پ ﭙ\n",
            "خ ﺦ\n",
            "و ﺅ\n",
            "چ ﭻ\n",
            "پ ﭖ\n",
            "ط ﻁ\n",
            "گ ﮒ\n",
            "  ī\n",
            "  ü\n",
            "ﷲ ﷲ\n",
            "  ł\n",
            "  ø\n",
            "ک ګ\n",
            "N ²\n",
            "ی ﻰ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RfiBW95sJU-",
        "outputId": "a252176f-9c6a-46e4-8f64-7c0ed137d25a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# char dic !\n",
        "chars_dictionary ={}\n",
        "for i in range(len(charss)):\n",
        "    chars_dictionary[chars_old[i]] = charss[i]\n",
        "print(chars_dictionary)\n",
        "print(chars_dictionary.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'گ': 'گ', 'ز': 'ز', 'ا': 'ا', 'ر': 'ر', 'ش': 'ش', 'خ': 'خ', 'ب': 'ب', 'ن': 'ن', 'س': 'س', 'ی': 'ی', 'ه': 'ه', 'و': 'و', 'ل': 'ل', 'ق': 'ق', 'م': 'م', 'ح': 'ح', 'پ': 'پ', 'د': 'د', 'ئ': 'ی', 'ک': 'ک', 'N': 'N', 'ف': 'ف', 'ت': 'ت', '.': '.', 'ج': 'ج', 'ع': 'ع', 'چ': 'چ', 'ط': 'ط', 'ض': 'ض', 'غ': 'غ', 'ص': 'ص', 'ظ': 'ظ', 'ذ': 'ذ', 'آ': 'آ', 'ث': 'ث', 'ي': 'ی', 'ژ': 'ژ', 'ك': 'ک', 'ؤ': 'و', 'أ': 'ا', 'ء': ' ', 'ـ': ' ', '؟': '؟', 'ۀ': 'ه', 'ى': 'ى', 'إ': 'ا', 'ة': 'ه', 'ˈ': ' ', 'é': ' ', 'ﻫ': 'ه', 'ﻤ': 'م', 'ﻴ': 'ی', 'ﻠ': 'ل', 'ﺘ': 'ت', 'ﻮ': 'و', 'ﻥ': 'ن', 'ﺸ': 'ش', 'ﻪ': 'ه', 'ﺑ': 'ب', 'ﻬ': 'ه', 'ﺮ': 'ر', 'ﻳ': 'ی', 'ﻦ': 'ن', 'ﻧ': 'ن', 'ﺠ': 'ج', 'ﺭ': 'ر', 'ﺍ': 'ا', 'ﯽ': 'ی', 'ﺗ': 'ت', 'ﮐ': 'ک', 'ﺴ': 'س', 'ﺐ': 'ب', 'ﺩ': 'د', '½': 'N', '¼': 'N', 'ہ': 'ه', 'ۆ': 'و', 'ﻣ': 'م', 'ﻨ': 'ن', 'ﮕ': 'گ', 'ﺳ': 'س', 'ﻞ': 'ل', 'ﯿ': 'ی', 'ﻝ': 'ل', 'ﻌ': 'ع', 'ﺯ': 'ز', 'ﮑ': 'ک', 'ﻼ': 'لا', 'ﺕ': 'ت', 'ﺒ': 'ب', 'ﻓ': 'ف', 'ں': ' ', 'ﺎ': 'ا', 'ﻟ': 'ل', 'ﺷ': 'ش', 'ﯾ': 'ی', 'ﯼ': 'ی', 'ﻄ': 'ط', 'ﻘ': 'ق', 'ﺣ': 'ح', 'ﻏ': 'غ', 'ﻕ': 'ق', 'ﺪ': 'د', 'ﺧ': 'خ', 'ﺝ': 'ج', 'ﻋ': 'ع', 'ﺖ': 'ت', 'ﻃ': 'ط', 'ﺡ': 'ح', 'ﻭ': 'و', 'ﺁ': 'آ', 'ﻔ': 'ف', 'ﺰ': 'ز', 'ﺟ': 'ج', 'ﻡ': 'م', 'ﻩ': 'ه', 'ﭼ': 'چ', 'ﭘ': 'پ', 'ﺞ': 'ج', 'ﻗ': 'ق', 'ﻇ': 'ظ', 'ﻒ': 'ف', 'ﮔ': 'گ', 'ﻛ': 'ک', 'ە': 'ه', 'ۯ': 'ر', 'ā': ' ', 'М': ' ', 'о': ' ', 'н': ' ', 'г': ' ', 'л': ' ', 'у': ' ', 'с': ' ', 'Ө': ' ', 'и': ' ', 'й': ' ', 'Б': ' ', 'а': ' ', 'Ч': ' ', 'б': ' ', 'Э': ' ', 'р': ' ', 'д': ' ', 'э': ' ', 'т': ' ', 'ó': ' ', 'ﺤ': 'ح', 'ﻜ': 'ک', 'ﻢ': 'م', 'ﺛ': 'ژ', 'ﻉ': 'ع', 'ﻀ': 'ض', 'ﺻ': 'ص', 'ﺢ': 'ح', 'ﻂ': 'ط', 'č': ' ', 'ۃ': 'ه', 'ï': ' ', 'â': ' ', 'ğ': ' ', 'ھ': 'ه', 'ﺏ': 'ب', 'ﮎ': 'ک', 'ﺨ': 'خ', 'ﺼ': 'ص', 'ﺹ': 'ص', 'ﺬ': 'ذ', 'ﺵ': 'ش', 'ﻯ': 'ی', 'æ': ' ', 'ɔ': ' ', 'ɪ': ' ', 'ﻈ': 'ظ', 'ﻖ': 'ق', 'ä': ' ', 'ć': ' ', 'á': ' ', 'è': ' ', 'ö': ' ', 'ﺿ': 'ض', 'ﺲ': 'س', 'ﺋ': 'ی', 'ﮏ': 'ک', 'ڗ': 'ژ', 'ﺶ': 'ش', 'ﻲ': 'ی', 'ã': ' ', 'ē': ' ', 'í': ' ', 'ə': ' ', 'Á': ' ', 'ñ': ' ', 'ﺆ': 'و', 'ﭽ': 'چ', 'ﻐ': 'غ', 'ﺱ': 'س', 'ﺌ': 'ی', 'ﻍ': 'غ', 'ﺄ': 'ا', 'ﻻ': 'لا', 'ﮋ': 'ژ', 'ﻊ': 'ع', 'ﭗ': 'پ', 'ﺜ': 'ث', 'ﻑ': 'ف', 'α': ' ', 'ﺥ': 'خ', 'ﻙ': 'ک', 'ﭙ': 'پ', 'ﺦ': 'خ', 'ﺅ': 'و', 'ﭻ': 'چ', 'ﭖ': 'پ', 'ﻁ': 'ط', 'ﮒ': 'گ', 'ī': ' ', 'ü': ' ', 'ﷲ': 'ﷲ', 'ł': ' ', 'ø': ' ', 'ګ': 'ک', '²': 'N', 'ﻰ': 'ی'}\n",
            "dict_keys(['گ', 'ز', 'ا', 'ر', 'ش', 'خ', 'ب', 'ن', 'س', 'ی', 'ه', 'و', 'ل', 'ق', 'م', 'ح', 'پ', 'د', 'ئ', 'ک', 'N', 'ف', 'ت', '.', 'ج', 'ع', 'چ', 'ط', 'ض', 'غ', 'ص', 'ظ', 'ذ', 'آ', 'ث', 'ي', 'ژ', 'ك', 'ؤ', 'أ', 'ء', 'ـ', '؟', 'ۀ', 'ى', 'إ', 'ة', 'ˈ', 'é', 'ﻫ', 'ﻤ', 'ﻴ', 'ﻠ', 'ﺘ', 'ﻮ', 'ﻥ', 'ﺸ', 'ﻪ', 'ﺑ', 'ﻬ', 'ﺮ', 'ﻳ', 'ﻦ', 'ﻧ', 'ﺠ', 'ﺭ', 'ﺍ', 'ﯽ', 'ﺗ', 'ﮐ', 'ﺴ', 'ﺐ', 'ﺩ', '½', '¼', 'ہ', 'ۆ', 'ﻣ', 'ﻨ', 'ﮕ', 'ﺳ', 'ﻞ', 'ﯿ', 'ﻝ', 'ﻌ', 'ﺯ', 'ﮑ', 'ﻼ', 'ﺕ', 'ﺒ', 'ﻓ', 'ں', 'ﺎ', 'ﻟ', 'ﺷ', 'ﯾ', 'ﯼ', 'ﻄ', 'ﻘ', 'ﺣ', 'ﻏ', 'ﻕ', 'ﺪ', 'ﺧ', 'ﺝ', 'ﻋ', 'ﺖ', 'ﻃ', 'ﺡ', 'ﻭ', 'ﺁ', 'ﻔ', 'ﺰ', 'ﺟ', 'ﻡ', 'ﻩ', 'ﭼ', 'ﭘ', 'ﺞ', 'ﻗ', 'ﻇ', 'ﻒ', 'ﮔ', 'ﻛ', 'ە', 'ۯ', 'ā', 'М', 'о', 'н', 'г', 'л', 'у', 'с', 'Ө', 'и', 'й', 'Б', 'а', 'Ч', 'б', 'Э', 'р', 'д', 'э', 'т', 'ó', 'ﺤ', 'ﻜ', 'ﻢ', 'ﺛ', 'ﻉ', 'ﻀ', 'ﺻ', 'ﺢ', 'ﻂ', 'č', 'ۃ', 'ï', 'â', 'ğ', 'ھ', 'ﺏ', 'ﮎ', 'ﺨ', 'ﺼ', 'ﺹ', 'ﺬ', 'ﺵ', 'ﻯ', 'æ', 'ɔ', 'ɪ', 'ﻈ', 'ﻖ', 'ä', 'ć', 'á', 'è', 'ö', 'ﺿ', 'ﺲ', 'ﺋ', 'ﮏ', 'ڗ', 'ﺶ', 'ﻲ', 'ã', 'ē', 'í', 'ə', 'Á', 'ñ', 'ﺆ', 'ﭽ', 'ﻐ', 'ﺱ', 'ﺌ', 'ﻍ', 'ﺄ', 'ﻻ', 'ﮋ', 'ﻊ', 'ﭗ', 'ﺜ', 'ﻑ', 'α', 'ﺥ', 'ﻙ', 'ﭙ', 'ﺦ', 'ﺅ', 'ﭻ', 'ﭖ', 'ﻁ', 'ﮒ', 'ī', 'ü', 'ﷲ', 'ł', 'ø', 'ګ', '²', 'ﻰ'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmETVxvH9S47"
      },
      "source": [
        "with open('rep_chars', 'w') as f:\n",
        "            for i in chars_dictionary.keys():\n",
        "                string = i+ ', ' + chars_dictionary[i] + '\\n'\n",
        "                f.write(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__R1vmkHiWGo"
      },
      "source": [
        "with open('chars.txt', 'w') as f:\n",
        "            for i in c.dictionary_char.keys():\n",
        "                string = i + '\\n'\n",
        "                f.write(string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFX9bNmh-FpT"
      },
      "source": [
        "file_char = open(\"rep_chars\", \"r\")\n",
        "chs = file_char.read()\n",
        "chs = [each.split(', ') for each in (chs.split('\\n'))]\n",
        "chars_dictionary_ = {}\n",
        "for ch in chs:\n",
        "    chars_dictionary_[ch[0]] = ch[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAaOI1uC-RxW",
        "outputId": "108c8ca1-39d9-4624-c130-c3050f8d9a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chars_dictionary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': '.',\n",
              " 'N': 'N',\n",
              " '²': 'N',\n",
              " '¼': 'N',\n",
              " '½': 'N',\n",
              " 'Á': ' ',\n",
              " 'á': ' ',\n",
              " 'â': ' ',\n",
              " 'ã': ' ',\n",
              " 'ä': ' ',\n",
              " 'æ': ' ',\n",
              " 'è': ' ',\n",
              " 'é': ' ',\n",
              " 'í': ' ',\n",
              " 'ï': ' ',\n",
              " 'ñ': ' ',\n",
              " 'ó': ' ',\n",
              " 'ö': ' ',\n",
              " 'ø': ' ',\n",
              " 'ü': ' ',\n",
              " 'ā': ' ',\n",
              " 'ć': ' ',\n",
              " 'č': ' ',\n",
              " 'ē': ' ',\n",
              " 'ğ': ' ',\n",
              " 'ī': ' ',\n",
              " 'ł': ' ',\n",
              " 'ɔ': ' ',\n",
              " 'ə': ' ',\n",
              " 'ɪ': ' ',\n",
              " 'ˈ': ' ',\n",
              " 'α': ' ',\n",
              " 'Б': ' ',\n",
              " 'М': ' ',\n",
              " 'Ч': ' ',\n",
              " 'Э': ' ',\n",
              " 'а': ' ',\n",
              " 'б': ' ',\n",
              " 'г': ' ',\n",
              " 'д': ' ',\n",
              " 'и': ' ',\n",
              " 'й': ' ',\n",
              " 'л': ' ',\n",
              " 'н': ' ',\n",
              " 'о': ' ',\n",
              " 'р': ' ',\n",
              " 'с': ' ',\n",
              " 'т': ' ',\n",
              " 'у': ' ',\n",
              " 'э': ' ',\n",
              " 'Ө': ' ',\n",
              " '؟': '؟',\n",
              " 'ء': ' ',\n",
              " 'آ': 'آ',\n",
              " 'أ': 'ا',\n",
              " 'ؤ': 'و',\n",
              " 'إ': 'ا',\n",
              " 'ئ': 'ی',\n",
              " 'ا': 'ا',\n",
              " 'ب': 'ب',\n",
              " 'ة': 'ه',\n",
              " 'ت': 'ت',\n",
              " 'ث': 'ث',\n",
              " 'ج': 'ج',\n",
              " 'ح': 'ح',\n",
              " 'خ': 'خ',\n",
              " 'د': 'د',\n",
              " 'ذ': 'ذ',\n",
              " 'ر': 'ر',\n",
              " 'ز': 'ز',\n",
              " 'س': 'س',\n",
              " 'ش': 'ش',\n",
              " 'ص': 'ص',\n",
              " 'ض': 'ض',\n",
              " 'ط': 'ط',\n",
              " 'ظ': 'ظ',\n",
              " 'ع': 'ع',\n",
              " 'غ': 'غ',\n",
              " 'ـ': ' ',\n",
              " 'ف': 'ف',\n",
              " 'ق': 'ق',\n",
              " 'ك': 'ک',\n",
              " 'ل': 'ل',\n",
              " 'م': 'م',\n",
              " 'ن': 'ن',\n",
              " 'ه': 'ه',\n",
              " 'و': 'و',\n",
              " 'ى': 'ى',\n",
              " 'ي': 'ی',\n",
              " 'پ': 'پ',\n",
              " 'چ': 'چ',\n",
              " 'ڗ': 'ژ',\n",
              " 'ژ': 'ژ',\n",
              " 'ک': 'ک',\n",
              " 'ګ': 'ک',\n",
              " 'گ': 'گ',\n",
              " 'ں': ' ',\n",
              " 'ھ': 'ه',\n",
              " 'ۀ': 'ه',\n",
              " 'ہ': 'ه',\n",
              " 'ۃ': 'ه',\n",
              " 'ۆ': 'و',\n",
              " 'ی': 'ی',\n",
              " 'ە': 'ه',\n",
              " 'ۯ': 'ر',\n",
              " 'ﭖ': 'پ',\n",
              " 'ﭗ': 'پ',\n",
              " 'ﭘ': 'پ',\n",
              " 'ﭙ': 'پ',\n",
              " 'ﭻ': 'چ',\n",
              " 'ﭼ': 'چ',\n",
              " 'ﭽ': 'چ',\n",
              " 'ﮋ': 'ژ',\n",
              " 'ﮎ': 'ک',\n",
              " 'ﮏ': 'ک',\n",
              " 'ﮐ': 'ک',\n",
              " 'ﮑ': 'ک',\n",
              " 'ﮒ': 'گ',\n",
              " 'ﮔ': 'گ',\n",
              " 'ﮕ': 'گ',\n",
              " 'ﯼ': 'ی',\n",
              " 'ﯽ': 'ی',\n",
              " 'ﯾ': 'ی',\n",
              " 'ﯿ': 'ی',\n",
              " 'ﷲ': 'ﷲ',\n",
              " 'ﺁ': 'آ',\n",
              " 'ﺄ': 'ا',\n",
              " 'ﺅ': 'و',\n",
              " 'ﺆ': 'و',\n",
              " 'ﺋ': 'ی',\n",
              " 'ﺌ': 'ی',\n",
              " 'ﺍ': 'ا',\n",
              " 'ﺎ': 'ا',\n",
              " 'ﺏ': 'ب',\n",
              " 'ﺐ': 'ب',\n",
              " 'ﺑ': 'ب',\n",
              " 'ﺒ': 'ب',\n",
              " 'ﺕ': 'ت',\n",
              " 'ﺖ': 'ت',\n",
              " 'ﺗ': 'ت',\n",
              " 'ﺘ': 'ت',\n",
              " 'ﺛ': 'ژ',\n",
              " 'ﺜ': 'ث',\n",
              " 'ﺝ': 'ج',\n",
              " 'ﺞ': 'ج',\n",
              " 'ﺟ': 'ج',\n",
              " 'ﺠ': 'ج',\n",
              " 'ﺡ': 'ح',\n",
              " 'ﺢ': 'ح',\n",
              " 'ﺣ': 'ح',\n",
              " 'ﺤ': 'ح',\n",
              " 'ﺥ': 'خ',\n",
              " 'ﺦ': 'خ',\n",
              " 'ﺧ': 'خ',\n",
              " 'ﺨ': 'خ',\n",
              " 'ﺩ': 'د',\n",
              " 'ﺪ': 'د',\n",
              " 'ﺬ': 'ذ',\n",
              " 'ﺭ': 'ر',\n",
              " 'ﺮ': 'ر',\n",
              " 'ﺯ': 'ز',\n",
              " 'ﺰ': 'ز',\n",
              " 'ﺱ': 'س',\n",
              " 'ﺲ': 'س',\n",
              " 'ﺳ': 'س',\n",
              " 'ﺴ': 'س',\n",
              " 'ﺵ': 'ش',\n",
              " 'ﺶ': 'ش',\n",
              " 'ﺷ': 'ش',\n",
              " 'ﺸ': 'ش',\n",
              " 'ﺹ': 'ص',\n",
              " 'ﺻ': 'ص',\n",
              " 'ﺼ': 'ص',\n",
              " 'ﺿ': 'ض',\n",
              " 'ﻀ': 'ض',\n",
              " 'ﻁ': 'ط',\n",
              " 'ﻂ': 'ط',\n",
              " 'ﻃ': 'ط',\n",
              " 'ﻄ': 'ط',\n",
              " 'ﻇ': 'ظ',\n",
              " 'ﻈ': 'ظ',\n",
              " 'ﻉ': 'ع',\n",
              " 'ﻊ': 'ع',\n",
              " 'ﻋ': 'ع',\n",
              " 'ﻌ': 'ع',\n",
              " 'ﻍ': 'غ',\n",
              " 'ﻏ': 'غ',\n",
              " 'ﻐ': 'غ',\n",
              " 'ﻑ': 'ف',\n",
              " 'ﻒ': 'ف',\n",
              " 'ﻓ': 'ف',\n",
              " 'ﻔ': 'ف',\n",
              " 'ﻕ': 'ق',\n",
              " 'ﻖ': 'ق',\n",
              " 'ﻗ': 'ق',\n",
              " 'ﻘ': 'ق',\n",
              " 'ﻙ': 'ک',\n",
              " 'ﻛ': 'ک',\n",
              " 'ﻜ': 'ک',\n",
              " 'ﻝ': 'ل',\n",
              " 'ﻞ': 'ل',\n",
              " 'ﻟ': 'ل',\n",
              " 'ﻠ': 'ل',\n",
              " 'ﻡ': 'م',\n",
              " 'ﻢ': 'م',\n",
              " 'ﻣ': 'م',\n",
              " 'ﻤ': 'م',\n",
              " 'ﻥ': 'ن',\n",
              " 'ﻦ': 'ن',\n",
              " 'ﻧ': 'ن',\n",
              " 'ﻨ': 'ن',\n",
              " 'ﻩ': 'ه',\n",
              " 'ﻪ': 'ه',\n",
              " 'ﻫ': 'ه',\n",
              " 'ﻬ': 'ه',\n",
              " 'ﻭ': 'و',\n",
              " 'ﻮ': 'و',\n",
              " 'ﻯ': 'ی',\n",
              " 'ﻰ': 'ی',\n",
              " 'ﻲ': 'ی',\n",
              " 'ﻳ': 'ی',\n",
              " 'ﻴ': 'ی',\n",
              " 'ﻻ': 'لا',\n",
              " 'ﻼ': 'لا'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TG_-9cPe_Xr",
        "outputId": "6d032e18-4258-4c3c-ec24-413f996ed33f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# len(c.dictionary_char.keys())\n",
        "for i in c.dictionary_char.keys():\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "گ 35840\n",
            "ز 40697\n",
            "ا 335199\n",
            "ر 179716\n",
            "ش 55008\n",
            "خ 25289\n",
            "ب 74847\n",
            "ن 166900\n",
            "س 76285\n",
            "ی 201085\n",
            "ه 109706\n",
            "و 131205\n",
            "ل 79664\n",
            "ق 23647\n",
            "م 126396\n",
            "ح 21662\n",
            "پ 28476\n",
            "د 119692\n",
            "ئ 6240\n",
            "ک 49644\n",
            "N 10259\n",
            "ف 39331\n",
            "ت 107243\n",
            ". 18558\n",
            "ج 25346\n",
            "ع 28685\n",
            "چ 8399\n",
            "ط 9832\n",
            "ض 6727\n",
            "غ 6688\n",
            "ص 14180\n",
            "ظ 3657\n",
            "ذ 5236\n",
            "آ 14889\n",
            "ث 3150\n",
            "ي 65323\n",
            "ژ 3421\n",
            "ك 13447\n",
            "ؤ 506\n",
            "أ 2860\n",
            "ء 1298\n",
            "ـ 3738\n",
            "؟ 893\n",
            "ۀ 512\n",
            "ى 8308\n",
            "إ 383\n",
            "ة 1587\n",
            "ˈ 69\n",
            "é 3\n",
            "ﻫ 70\n",
            "ﻤ 71\n",
            "ﻴ 53\n",
            "ﻠ 38\n",
            "ﺘ 84\n",
            "ﻮ 201\n",
            "ﻥ 63\n",
            "ﺸ 35\n",
            "ﻪ 103\n",
            "ﺑ 127\n",
            "ﻬ 31\n",
            "ﺮ 219\n",
            "ﻳ 43\n",
            "ﻦ 44\n",
            "ﻧ 142\n",
            "ﺠ 12\n",
            "ﺭ 144\n",
            "ﺍ 216\n",
            "ﯽ 98\n",
            "ﺗ 75\n",
            "ﮐ 37\n",
            "ﺴ 46\n",
            "ﺐ 17\n",
            "ﺩ 150\n",
            "½ 2\n",
            "¼ 1\n",
            "ہ 2\n",
            "ۆ 14\n",
            "ﻣ 204\n",
            "ﻨ 108\n",
            "ﮕ 31\n",
            "ﺳ 105\n",
            "ﻞ 27\n",
            "ﯿ 111\n",
            "ﻝ 16\n",
            "ﻌ 24\n",
            "ﺯ 58\n",
            "ﮑ 28\n",
            "ﻼ 15\n",
            "ﺕ 16\n",
            "ﺒ 46\n",
            "ﻓ 52\n",
            "ں 1\n",
            "ﺎ 357\n",
            "ﻟ 40\n",
            "ﺷ 65\n",
            "ﯾ 104\n",
            "ﯼ 41\n",
            "ﻄ 8\n",
            "ﻘ 19\n",
            "ﺣ 32\n",
            "ﻏ 7\n",
            "ﻕ 3\n",
            "ﺪ 149\n",
            "ﺧ 49\n",
            "ﺝ 3\n",
            "ﻋ 40\n",
            "ﺖ 55\n",
            "ﻃ 11\n",
            "ﺡ 4\n",
            "ﻭ 65\n",
            "ﺁ 26\n",
            "ﻔ 17\n",
            "ﺰ 30\n",
            "ﺟ 43\n",
            "ﻡ 45\n",
            "ﻩ 36\n",
            "ﭼ 16\n",
            "ﭘ 26\n",
            "ﺞ 1\n",
            "ﻗ 40\n",
            "ﻇ 4\n",
            "ﻒ 6\n",
            "ﮔ 44\n",
            "ﻛ 33\n",
            "ە 8\n",
            "ۯ 1\n",
            "ā 4\n",
            "М 1\n",
            "о 3\n",
            "н 4\n",
            "г 3\n",
            "л 5\n",
            "у 2\n",
            "с 2\n",
            "Ө 1\n",
            "и 1\n",
            "й 2\n",
            "Б 1\n",
            "а 3\n",
            "Ч 1\n",
            "б 1\n",
            "Э 1\n",
            "р 1\n",
            "д 1\n",
            "э 2\n",
            "т 1\n",
            "ó 1\n",
            "ﺤ 13\n",
            "ﻜ 8\n",
            "ﻢ 52\n",
            "ﺛ 4\n",
            "ﻉ 3\n",
            "ﻀ 5\n",
            "ﺻ 13\n",
            "ﺢ 4\n",
            "ﻂ 6\n",
            "č 1\n",
            "ۃ 1\n",
            "ï 2\n",
            "â 3\n",
            "ğ 1\n",
            "ھ 13\n",
            "ﺏ 10\n",
            "ﮎ 1\n",
            "ﺨ 8\n",
            "ﺼ 12\n",
            "ﺹ 2\n",
            "ﺬ 8\n",
            "ﺵ 10\n",
            "ﻯ 1\n",
            "æ 1\n",
            "ɔ 1\n",
            "ɪ 1\n",
            "ﻈ 6\n",
            "ﻖ 3\n",
            "ä 1\n",
            "ć 1\n",
            "á 1\n",
            "è 1\n",
            "ö 1\n",
            "ﺿ 7\n",
            "ﺲ 3\n",
            "ﺋ 6\n",
            "ﮏ 6\n",
            "ڗ 1\n",
            "ﺶ 8\n",
            "ﻲ 2\n",
            "ã 1\n",
            "ē 1\n",
            "í 1\n",
            "ə 2\n",
            "Á 1\n",
            "ñ 1\n",
            "ﺆ 3\n",
            "ﭽ 9\n",
            "ﻐ 2\n",
            "ﺱ 2\n",
            "ﺌ 3\n",
            "ﻍ 2\n",
            "ﺄ 1\n",
            "ﻻ 6\n",
            "ﮋ 1\n",
            "ﻊ 2\n",
            "ﭗ 2\n",
            "ﺜ 1\n",
            "ﻑ 1\n",
            "α 1\n",
            "ﺥ 1\n",
            "ﻙ 1\n",
            "ﭙ 3\n",
            "ﺦ 1\n",
            "ﺅ 1\n",
            "ﭻ 1\n",
            "ﭖ 1\n",
            "ﻁ 2\n",
            "ﮒ 1\n",
            "ī 1\n",
            "ü 1\n",
            "ﷲ 1\n",
            "ł 1\n",
            "ø 1\n",
            "ګ 3\n",
            "² 4\n",
            "ﻰ 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atLVmU7sLQQK"
      },
      "source": [
        "## Using the Corpus class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSa8FNZurKjF"
      },
      "source": [
        "### training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "insLCIZ6YZPS",
        "outputId": "69cf9db8-b400-4425-e7d3-970f2ded789e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "new = X_train#.iloc[:1000]\n",
        "new_y = y_train#.iloc[:1000]\n",
        "c = Corpus(new)\n",
        "all_tokens = c.number_of_all_tokens()\n",
        "unique_tokens = c.number_of_all_unique_tokens()\n",
        "print('Number of all words: ', all_tokens)\n",
        "print('Number of all unique words: ', unique_tokens)\n",
        "percentage = ((all_tokens-(c.UNK_count))/all_tokens)*100\n",
        "print(f'Percentage of most frequent: {percentage}%')\n",
        "\n",
        "print(len(c.corpus))\n",
        "print(len(c.word2index))\n",
        "print(len(c.index2word))\n",
        "print(len(c.dictionary))\n",
        "print(len(c.most_frequent_words))\n",
        "\n",
        "remain_X, remain_y = c.less_than_ave(new_y)\n",
        "tokenized = c.tokenize(level=0)   # I didn't use tokenized!\n",
        "\n",
        "y = pd.DataFrame(remain_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of all words:  16085209\n",
            "Number of all unique words:  268406\n",
            "Percentage of most frequent: 90.81640157737459%\n",
            "82007\n",
            "10002\n",
            "10002\n",
            "268406\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyK6mV4X3zlf"
      },
      "source": [
        "Relating an index for each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4StFAQLtaRQt"
      },
      "source": [
        "labels = (np.unique(y))\n",
        "\n",
        "# label to index\n",
        "label_dict = {}\n",
        "for i in range(len(labels)):\n",
        "    label_dict[labels[i]]=i\n",
        "\n",
        "# index to label\n",
        "label_dict_= {label_dict[l]:l for l in label_dict.keys()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9xpkGqrYU6t",
        "outputId": "e2a18461-6cf0-4a01-f37e-10f5b1c6ece7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in label_dict.keys():\n",
        "    print(i, label_dict[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "اجتماعی 0\n",
            "اقتصادی 1\n",
            "بین‌الملل 2\n",
            "سیاسی 3\n",
            "علمی پزشکی 4\n",
            "فرهنگی هنری 5\n",
            "فضای مجازی 6\n",
            "فیلم و صوت  7\n",
            "وب‌گردی 8\n",
            "ورزشی 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5AIY6xx4Bi1"
      },
      "source": [
        "Encode y to index of each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kwsDceq3p6h",
        "outputId": "10993441-ac58-4c70-d906-5ae847876a33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_encoded =[]\n",
        "for idx in range(y.shape[0]):\n",
        "    y_encoded.append(label_dict[y[0][idx]])\n",
        "print(len(y_encoded))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvsQf7Lxe0NC",
        "outputId": "1000bd6d-406e-4aa0-9c39-2829565591f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_row = 151\n",
        "print(remain_X[check_row])\n",
        "print(y_encoded[check_row])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['گزارشسرویس', 'بینالملل', 'باشگاه', 'خبرنگارانشبکه', 'العربیه', 'لحظاتی', 'گزارش', 'داد', 'وزارت', 'خزانهداری', 'آمریکا', 'تحریمهایی', 'UNK', 'UNK', 'سخنگوی', 'گروهک', 'داعش', 'سعید', 'عارف', 'عناصر', 'جبهه', 'النصره', 'اعمال', 'UNK', 'تحریم', 'آمریکا', 'دو', 'تن', 'اعضای', 'داعش', 'النصره', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ3BaIBXrHm8"
      },
      "source": [
        "### validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvl-32jirC3F",
        "outputId": "d49d2575-1044-43ef-be36-0acd1f680bf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "XVal = X_val#.iloc[:100]\n",
        "yVal = y_val#.iloc[:100]\n",
        "c_val = Corpus(XVal, train=False)\n",
        "all_tokens = c_val.number_of_all_tokens()\n",
        "unique_tokens = c_val.number_of_all_unique_tokens()\n",
        "tokenized_val = c_val.tokenize(level=0)\n",
        "print(len(c_val.corpus))\n",
        "print(len(c_val.dictionary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35146\n",
            "159452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrjE-76O4t8s"
      },
      "source": [
        "XVal = c_val.corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dySGFdhEtAho",
        "outputId": "a941cf9e-60e4-4e33-bed4-516efd6f2a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(yVal))\n",
        "print(type(yVal))\n",
        "yVal = yVal.values.tolist()\n",
        "print(type(yVal))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35146\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeQx6EiosuzA",
        "outputId": "6315d127-dad8-465f-ec33-472d5fd41507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Encode y to class index\n",
        "for idx in range(len(yVal)):\n",
        "    yVal[idx]=label_dict[yVal[idx]] \n",
        "print(len(yVal))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8J_XuXH1G2x",
        "outputId": "88dfe7b3-3180-4283-c055-ab0c0de0c37b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "check_row = 51\n",
        "print(XVal[check_row])\n",
        "print(yVal[check_row], label_dict_[yVal[check_row]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['آرش', 'پیک', 'دبیر', 'انجمن', 'اسلامی', 'دانشکده', 'مهندسی', 'دانشگاه', 'شهید', 'باهنر', 'شیراز', 'گفتگو', 'خبرنگار', 'دانشگاه', 'باشگاه', 'خبرنگاران', 'برگزاری', 'نشستی', 'موضوع', 'جریانشناسی', 'دفتر', 'تحکیم', 'وحدت', 'خبرداد', 'نشست', 'پرسش', 'پاسخ', 'دانشجویی', 'همراه', 'است', 'پیشنه', 'اهداف', 'دفتر', 'تحکیم', 'وحدت', 'بررسی', 'خواهد', '.وی', 'اضافه', 'برنامه', 'یکشنبه', 'N', 'آبان', 'دانشکده', 'مهندسی', 'دانشگاه', 'باهنر', 'برگزار', 'شده', 'مهدی', 'امیریان', 'دبیر', 'دفتر', 'تحکیم', 'وحدت', 'جلسه', 'حضور', 'خواهد', '.', 'انتهای', 'پیامجلسه', 'پرسش', 'پاسخ', 'دانشجویی', 'موضوع', 'جریانشناسی', 'دفتر', 'تحکیم', 'وحدت']\n",
            "3 سیاسی\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq1vWrD4ZrjA"
      },
      "source": [
        "# Classifier - Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEx7nz4xbqzq"
      },
      "source": [
        "class NaiveBayes():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.words_count_class = []\n",
        "        self.X = 0\n",
        "        self.y = 0\n",
        "        self.classes = 0        # number of classes\n",
        "        self.count_classes = [] # list of number of samples in each classes\n",
        "        self.sum_classes =[]    # list of number of words in each classes\n",
        "        self.term = []          # probability of each classes\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.classes = len(np.unique(self.y))\n",
        "        self.__create_words_count_class() \n",
        "        self.sum_classes = [sum(word_count.values()) for word_count in self.words_count_class]\n",
        "        self.__count_classes()\n",
        "        self.term = [np.log((i)/(sum(self.count_classes))) for i in self.count_classes]\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        self.__count_classes()\n",
        "        results =[]\n",
        "        for news in X_test:\n",
        "            results.append(self.__pred(news))\n",
        "        return results\n",
        "    \n",
        "    def __pred(self, x):\n",
        "        p = self.term[:]\n",
        "        new_words, count_unique = self.__new_words_dict(x)\n",
        "        for word in x:\n",
        "            for cl in range(self.classes):\n",
        "                if word in self.words_count_class[cl].keys():\n",
        "                    p[cl] += np.log(self.words_count_class[cl][word]/(self.sum_classes[cl]+count_unique[cl]))\n",
        "                else:\n",
        "                    p[cl] += np.log(1/(self.sum_classes[cl]+count_unique[cl]))\n",
        "        return np.array(p).argmax()\n",
        "\n",
        "    def evaluate(self, result, y):\n",
        "        c = 0\n",
        "        confusion_matrix = []\n",
        "        for i in range(self.classes):\n",
        "            confusion_matrix.append(10*[0])\n",
        "        for i in range(len(result)):\n",
        "            confusion_matrix[result[i]][y[i]] += 1\n",
        "            if result[i]==y[i]:\n",
        "                c += 1\n",
        "        acc = (c/len(result))*100\n",
        "        list1 = []\n",
        "        list2 = []\n",
        "        for i in range(len(confusion_matrix)):\n",
        "            list1.append(sum(confusion_matrix[i]))\n",
        "        for i in zip(*confusion_matrix):\n",
        "            list2.append(sum(i))\n",
        "        rec = 0 #[confusion_matrix[i][i]/(list1[i]) for i in range(self.classes)]\n",
        "        pre = 0 #[confusion_matrix[i][i]/(list2[i]) for i in range(self.classes)]\n",
        "        f1 = 0 #[(2*rec[i]*pre[i])/(rec[i]+pre[i]) for i in range(self.classes)]\n",
        "        # They caused zero division sometimes so I commented them!\n",
        "        return confusion_matrix, acc, pre, rec, f1\n",
        "\n",
        "    def __create_words_count_class(self):\n",
        "        for i in range(self.classes):\n",
        "            self.words_count_class.append({})\n",
        "        for row in range(len(self.X)):\n",
        "            for word in self.X[row]:\n",
        "                if word in self.words_count_class[self.y[row]]:\n",
        "                    self.words_count_class[self.y[row]][word] += 1\n",
        "                else:\n",
        "                    self.words_count_class[self.y[row]][word] = 1\n",
        "    \n",
        "    def __count_classes(self):\n",
        "        self.count_classes = [0]*self.classes\n",
        "        for i in range(self.classes):\n",
        "            self.count_classes[i] = self.y.count(i)\n",
        "\n",
        "    def __new_words_dict(self, x):\n",
        "        dic = []\n",
        "        for i in range(self.classes):\n",
        "            dic.append({})\n",
        "        for word in x:\n",
        "            for cl in range(self.classes):\n",
        "                if word not in self.words_count_class[cl].keys():\n",
        "                    if word in dic[cl].keys():\n",
        "                        dic[cl][word] += 1\n",
        "                    else: \n",
        "                        dic[cl][word] = 1\n",
        "        count_unique = [len(i) for i in dic]\n",
        "        return dic, count_unique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRu92SU-4SVi"
      },
      "source": [
        "Try these functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0GiLg86F_wz"
      },
      "source": [
        "nb = NaiveBayes()\n",
        "nb.fit(remain_X, y_encoded)\n",
        "prediction = nb.predict(XVal)\n",
        "confusion_matrix, acc, pre, rec, f1 = nb.evaluate(prediction, yVal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUeoekXtYzO9",
        "outputId": "013c9127-6799-4406-ef99-c369762ab9bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'Accuracy is {acc}%')\n",
        "print('Precision\\n', pre)\n",
        "print('Recall\\n', rec)\n",
        "print('F1\\n', f1)\n",
        "print('Confusion matrix')\n",
        "for idx, i in enumerate(confusion_matrix):\n",
        "    print(label_dict_[idx])\n",
        "    for j in i:\n",
        "        print(j, end='\\t')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is 70.2099812211916%\n",
            "Precision\n",
            " 0\n",
            "Recall\n",
            " 0\n",
            "F1\n",
            " 0\n",
            "Confusion matrix\n",
            "اجتماعی\n",
            "2526\t368\t37\t119\t147\t21\t2\t0\t31\t49\t\n",
            "اقتصادی\n",
            "33\t1930\t91\t27\t12\t1\t0\t0\t4\t10\t\n",
            "بین‌الملل\n",
            "25\t76\t6399\t258\t8\t4\t1\t0\t33\t53\t\n",
            "سیاسی\n",
            "254\t407\t266\t3725\t169\t59\t12\t0\t46\t114\t\n",
            "علمی پزشکی\n",
            "113\t111\t19\t48\t2514\t7\t0\t0\t16\t28\t\n",
            "فرهنگی هنری\n",
            "264\t105\t29\t121\t137\t3003\t10\t2\t65\t224\t\n",
            "فضای مجازی\n",
            "193\t303\t230\t49\t209\t40\t413\t42\t34\t264\t\n",
            "فیلم و صوت \n",
            "484\t686\t500\t278\t587\t653\t151\t82\t328\t1292\t\n",
            "وب‌گردی\n",
            "64\t0\t1\t0\t2\t0\t0\t0\t201\t1\t\n",
            "ورزشی\n",
            "4\t6\t10\t4\t8\t2\t0\t0\t9\t3883\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vScHktdULhhH",
        "outputId": "6db8c7f4-efbf-4503-814b-348a2c2c03fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.imshow(confusion_matrix)\n",
        "plt.colorbar(orientation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivId95NOhh3m"
      },
      "source": [
        "# Writing requested files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRfYoe0tTksP"
      },
      "source": [
        "c5 = Corpus(X_train, replace_frequent=False, train = False, char = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xan5WWJ6iUGk",
        "outputId": "a3dfebe8-82a8-4d90-dcc5-8444c15d7c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create words.txt\n",
        "print('Writing words in a text file: Wait...')\n",
        "with open('words.txt', 'w') as f:\n",
        "    for word in c5.dictionary:\n",
        "        string = word + '\\n'\n",
        "        f.write(string)\n",
        "print('Writing words in a text file: Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing words in a text file: Wait...\n",
            "Writing words in a text file: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmM3lK-Ajosk",
        "outputId": "2d0a7887-9e0a-463b-e4cf-a62f3b5d6cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create chars.txt\n",
        "print('Writing chars in a text file: Wait...')\n",
        "with open('chars.txt', 'w') as f:\n",
        "    for char in c5.dictionary_char:\n",
        "        string = char + '\\n'\n",
        "        f.write(string)\n",
        "print('Writing chars in a text file: Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing chars in a text file: Wait...\n",
            "Writing chars in a text file: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jpTggVlnIb_",
        "outputId": "8c287774-9fda-469e-913c-15a68090d4d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create most_frequent.txt\n",
        "print('Writing words in a text file: Wait...')\n",
        "with open('most_frequent.txt', 'w') as f:\n",
        "    for word in c.most_frequent_words:\n",
        "        string = word + '\\n'\n",
        "        f.write(string)\n",
        "print('Writing words in a text file: Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing words in a text file: Wait...\n",
            "Writing words in a text file: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYtqKIP0nWT5"
      },
      "source": [
        "# Create .pickle files for each dictionary\n",
        "import pickle\n",
        "\n",
        "# word2index.pickle\n",
        "with open('word2index.pickle', 'wb') as handle:\n",
        "    pickle.dump(c.word2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# index2word.pickle\n",
        "with open('index2word.pickle', 'wb') as handle:\n",
        "    pickle.dump(c.index2word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# char2index.pickle\n",
        "with open('char2index.pickle', 'wb') as handle:\n",
        "    pickle.dump(c5.char2index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# index2char.pickle\n",
        "with open('index2char.pickle', 'wb') as handle:\n",
        "    pickle.dump(c5.index2char, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKOM9TD9o2vV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}